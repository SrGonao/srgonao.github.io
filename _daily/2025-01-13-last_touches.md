---
layout: post
title: 2025-01-13
---

### Feature absorption

There seems to be almost no feature absorption on the MLP SAEs trained on Pythia. I wonder why that is.

### SAE overlap

Standard and Gated SAEs have more overlap that TopK SAEs, even though GPT-2 SAEs already have more overlap than Pythia ones.

Increasing the number of SAE seeds has a diminishing effect on the overlap. 

SAEs trained on more data have more overlap. 
