{"basics":{"Name":"Gon√ßalo Paulo","jobTitle":"Interpretability Researcher","email":"goncalo.paulo@eleuther.ai","url":"https://srgonao.github.io","summary":"I am a interpretability researcher at EleutherAI, currently working on sparse autoencoders.","location":{"address":"Lisbon, Portugal","postalCode":"","city":"Lisbon","countryCode":"PT","region":"Portugal"},"profiles":[{"network":"BlueSky","username":"goncalo-paulo.bsky.social","url":"https://bsky.app/profile/goncalo-paulo.bsky.social"}]},"work":[{"name":"EleutherAI","position":"Mechanistic Interpretability researcher","url":"https://eleuther.ai","startDate":"2024-07-01","summary":"Working on sparse autoencoders and automated interpretability.","highlights":["Sparse Autoencoders","Automated Interpretability"]}],"education":[{"institution":"Sapienza University of Rome, Rome, Italy","location":"Rome, Italy","url":"https://www.sapienza.it/","area":"Theoretical and Applied Mechanics","studyType":"PhD","startDate":"2020-11-01","endDate":"2023-11-01","score":"Cum Laude","courses":["Molecular Dynamics","Hydrophobic materials","Rare events"]}],"publications":[{"name":"Automatically Interpreting Millions of Features in Large Language Models","publisher":"ArXiv","releaseDate":"2024-10-17","url":"https://arxiv.org/abs/2410.13928","summary":"While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. However, these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language explanations for SAE features using LLMs. We test our framework on SAEs of varying sizes, activation functions, and losses, trained on two different open-weight LLMs. We introduce five new techniques to score the quality of explanations that are cheaper to run than the previous state of the art. One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods. We propose guidelines for generating better explanations that remain valid for a broader set of activating contexts, and discuss pitfalls with existing scoring techniques. We use our explanations to measure the semantic similarity of independently trained SAEs, and find that SAEs trained on nearby layers of the residual stream are highly similar. Our large-scale analysis confirms that SAE latents are indeed much more interpretable than neurons, even when neurons are sparsified using top-k postprocessing. Our code is available at this https URL, and our explanations are available at this https URL."}],"skills":[{"name":"Computational Physics","level":"PhD","icon":"fa-solid fa-hashtag","keywords":["Molecular Dynamics","Hydrophobic materials","Rare events"]}],"languages":[{"language":"Portuguese","fluency":"Native speaker","icon":""},{"language":"English","fluency":"Fluent","icon":""},{"language":"Italian","fluency":"Fluent","icon":""}],"interests":[{"name":"Interpretability","keywords":["Sparse Autoencoders","Automated Interpretability"]}],"projects":[{"name":"Automated Interpretability Repository","summary":"A repository for automated interpretability of sparse autoencoders.","highlights":["Sparse Autoencoders","Automated Interpretability"],"startDate":"2024-10-01","endDate":"","url":"https://github.com/EleutherAI/sae-auto-interp"}]}